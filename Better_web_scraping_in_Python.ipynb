{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Better web scraping in Python with Selenium, Beautiful Soup, and pandas\n",
    "\n",
    "[Source](https://medium.freecodecamp.org/better-web-scraping-in-python-with-selenium-beautiful-soup-and-pandas-d6390592e251)\n"
    "**Web Scraping**\n",
    "\n",
    "Using the Python programming language, it is possible to “scrape” data from the web in a quick and efficient manner.\n",
    "\n",
    "Web scraping is defined as:\n",
    "\n",
    "    a tool for turning the unstructured data on the web into machine readable, structured data which is ready for analysis. (source)\n",
    "\n",
    "Web scraping is a valuable tool in the data scientist’s skill set.\n",
    "\n",
    "*Now, what to scrape?*\n",
    "\n",
    "**Publicly Available Data**\n",
    "\n",
    "The KanView website supports “Transparency in Government”. That is also the slogan of the site. The site provides payroll data for the State of Kansas. And that’s great!\n",
    "\n",
    "Yet, like many government websites, it buries the data in drill-down links and tables. This often requires “best guess navigation” to find the specific data you are looking for. I wanted to use the public data provided for the universities within Kansas in a research project. Scraping the data with Python and saving it as JSON was what I needed to do to get started.\n",
    "JavaScript links increase the complexity\n",
    "\n",
    "Web scraping with Python often requires no more than the use of the Beautiful Soup module to reach the goal. Beautiful Soup is a popular Python library that makes web scraping by traversing the DOM (document object model) easier to implement.\n",
    "\n",
    "However, the KanView website uses JavaScript links. Therefore, examples using Python and Beautiful Soup will not work without some extra additions.\n",
    "\n",
    "**Selenium to the rescue**\n",
    "\n",
    "The Selenium package is used to automate web browser interaction from Python. With Selenium, programming a Python script to automate a web browser is possible. Afterwards, those pesky JavaScript links are no longer an issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selenium will now start a browser session. For Selenium to work, it must access the browser driver. By default, it will look in the same directory as the Python script. Links to Chrome, Firefox, Edge, and Safari drivers available here. The example code below uses Firefox:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#launch url\n",
    "url = \"http://kanview.ks.gov/PayRates/PayRates_Agency.aspx\"\n",
    "\n",
    "# create a new Firefox session\n",
    "driver = webdriver.Firefox()\n",
    "driver.implicitly_wait(30)\n",
    "driver.get(url)\n",
    "\n",
    "python_button = driver.find_element_by_id('MainContent_uxLevel1_Agencies_uxAgencyBtn_33') #FHSU\n",
    "python_button.click() #click fhsu link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `python_button.click()` above is telling Selenium to click the JavaScript link on the page. After arriving at the Job Titles page, Selenium hands off the page source to Beautiful Soup.\n",
    "\n",
    "**Transitioning to Beautiful Soup**\n",
    "\n",
    "Beautiful Soup remains the best way to traverse the DOM and scrape the data. After defining an empty list and a counter variable, it is time to ask Beautiful Soup to grab all the links on the page that match a regular expression, then Beautiful Soup will retrieve a JavaScript link for each job title at the state agency. Now in the code block of the for / in loop, Selenium will click each JavaScript link. Beautiful Soup will then retrieve the table from each page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selenium hands the page source to Beautiful Soup\n",
    "soup_level1=BeautifulSoup(driver.page_source, 'lxml')\n",
    "\n",
    "datalist = [] #empty list\n",
    "x = 0 #counter\n",
    "\n",
    "#Beautiful Soup grabs all Job Title links\n",
    "for link in soup_level1.find_all('a', id=re.compile(\"^MainContent_uxLevel2_JobTitles_uxJobTitleBtn_\")):\n",
    "    \n",
    "    #Selenium visits each Job Title page\n",
    "    python_button = driver.find_element_by_id('MainContent_uxLevel2_JobTitles_uxJobTitleBtn_' + str(x))\n",
    "    python_button.click() #click link\n",
    "    \n",
    "    #Selenium hands of the source of the specific job page to Beautiful Soup\n",
    "    soup_level2=BeautifulSoup(driver.page_source, 'lxml')\n",
    "\n",
    "    #Beautiful Soup grabs the HTML table on the page\n",
    "    table = soup_level2.find_all('table')[0]\n",
    "    \n",
    "    #Giving the HTML table to pandas to put in a dataframe object\n",
    "    df = pd.read_html(str(table),header=0)\n",
    "    \n",
    "    #Store the dataframe in a list\n",
    "    datalist.append(df[0])\n",
    "    \n",
    "    #Ask Selenium to click the back button\n",
    "    driver.execute_script(\"window.history.go(-1)\") \n",
    "    \n",
    "    #increment the counter variable before starting the loop over\n",
    "    x += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**pandas: Python Data Analysis Library**\n",
    "\n",
    "Beautiful Soup passes the findings to pandas. Pandas uses its `read_html` function to read the HTML table data into a dataframe. The dataframe is appended to the previously defined empty list.\n",
    "\n",
    "Before the code block of the loop is complete, Selenium needs to click the back button in the browser. This is so the next link in the loop will be available to click on the job listing page.\n",
    "\n",
    "When the for / in loop has completed, Selenium has visited every job title link. Beautiful Soup has retrieved the table from each page. Pandas has stored the data from each table in a dataframe. Each dataframe is an item in the datalist. The individual table dataframes must now merge into one large dataframe. The data will then be converted to JSON format with pandas.Dataframe.to_json:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loop has completed\n",
    "\n",
    "#end the Selenium browser session\n",
    "driver.quit()\n",
    "\n",
    "#combine all pandas dataframes in the list into one big dataframe\n",
    "result = pd.concat([pd.DataFrame(datalist[i]) for i in range(len(datalist))],ignore_index=True)\n",
    "\n",
    "#convert the pandas dataframe to JSON\n",
    "json_records = result.to_json(orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now Python creates the JSON data file. It is ready for use!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hungnv/Documents/Notebooks/fhsu_payroll_data.json\n"
     ]
    }
   ],
   "source": [
    "#get current working directory\n",
    "path = os.getcwd()\n",
    "\n",
    "#open, write, and close the file\n",
    "f = open(path + \"/fhsu_payroll_data.json\",\"w\") #FHSU\n",
    "f.write(json_records)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The automated process is fast**\n",
    "\n",
    "The automated web scraping process described above completes quickly. Selenium opens a browser window you can see working. This allows me to show you a screen capture video of how fast the process is. You see how fast the script follows a link, grabs the data, goes back, and clicks the next link. It makes retrieving the data from hundreds of links a matter of single-digit minutes.\n",
    "\n",
    "**Conclusion**\n",
    "\n",
    "Web scraping with Python and Beautiful Soup is an excellent tool to have within your skillset. Use web scraping when the data you need to work with is available to the public, but not necessarily conveniently available. When JavaScript provides or “hides” content, browser automation with Selenium will insure your code “sees” what you (as a user) should see. And finally, when you are scraping tables full of data, pandas is the Python data analysis library that will handle it all.\n",
    "\n",
    "**Reference:**\n",
    "\n",
    "The following article was a helpful reference for this project:\n",
    "\n",
    "https://pythonprogramminglanguage.com/web-scraping-with-pandas-and-beautifulsoup/\n",
    "\n",
    "Reach out to me any time on LinkedIn or Twitter. And if you liked this article, give it a few claps. I will sincerely appreciate it.\n",
    "\n",
    "https://www.linkedin.com/in/davidagray/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
